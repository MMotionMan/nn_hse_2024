{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9SGYChAd4KV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGZg1ROfd4KX"
      },
      "source": [
        "# Упражнение, для реализации \"Ванильной\" RNN\n",
        "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Kj-Tfqd4KY"
      },
      "outputs": [],
      "source": [
        "a = torch.ones((3,3))*3\n",
        "b = torch.ones((3,3))*5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCfNGbhDd4KZ",
        "outputId": "767550f7-4724-48ef-d045-62f877a1addb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[45., 45., 45.],\n",
              "        [45., 45., 45.],\n",
              "        [45., 45., 45.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "a @ b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD2aZH9od4KZ",
        "outputId": "4a0ce197-0288-4663-b6e5-a2ef8c3a34c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[15., 15., 15.],\n",
              "        [15., 15., 15.],\n",
              "        [15., 15., 15.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "a * b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHI6okjid4KZ"
      },
      "outputs": [],
      "source": [
        "word = 'ololoasdasddqweqw123456789'\n",
        "# word = 'hello'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWDHTRazd4KZ"
      },
      "source": [
        "## Датасет.\n",
        "Позволяет:\n",
        "* Закодировать символ при помощи one-hot\n",
        "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9FsTPRJd4Ka"
      },
      "outputs": [],
      "source": [
        "class WordDataSet:\n",
        "\n",
        "    def __init__(self, word):\n",
        "        self.chars2idx = {}\n",
        "        self.indexs  = []\n",
        "        for c in word:\n",
        "            if c not in self.chars2idx:\n",
        "                self.chars2idx[c] = len(self.chars2idx)\n",
        "\n",
        "            self.indexs.append(self.chars2idx[c])\n",
        "\n",
        "        self.vec_size = len(self.chars2idx)\n",
        "        self.seq_len  = len(word)\n",
        "\n",
        "    def get_one_hot(self, idx):\n",
        "        x = torch.zeros(self.vec_size)\n",
        "        x[idx] = 1\n",
        "        return x\n",
        "\n",
        "    def __iter__(self):\n",
        "        return zip(self.indexs[:-1], self.indexs[1:])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seq_len\n",
        "\n",
        "    def get_char_by_id(self, id):\n",
        "        for c, i in self.chars2idx.items():\n",
        "            if id == i: return c\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HhPFn1md4Ka"
      },
      "source": [
        "## Реализация базовой RNN\n",
        "<br/>\n",
        "Скрытый элемент\n",
        "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
        "Выход сети\n",
        "\n",
        "$$ y_t = W_{hy} h_t $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEFB0Pcjd4Ka"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
        "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
        "        self.activation  = nn.Tanh()\n",
        "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
        "\n",
        "    def forward(self, x, prev_hidden):\n",
        "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
        "#         Версия без активации - может происходить gradient exploding\n",
        "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
        "        output = self.outweight(hidden)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4hMLoWZd4Ka"
      },
      "source": [
        "## Инициализация переменных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "IQm75iQmd4Kb"
      },
      "outputs": [],
      "source": [
        "ds = WordDataSet(word=word)\n",
        "rnn = VanillaGRU(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "e_cnt     = 1000\n",
        "optim     = SGD(rnn.parameters(), lr = 0.05, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8VqJVgld4Kb"
      },
      "source": [
        "# Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB6e_Z72d4Kb",
        "outputId": "fe680526-af2d-4805-a7f4-c3b352f35a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72.83657836914062\n",
            "Clip gradient :  tensor(4.3963)\n",
            "65.51602935791016\n",
            "Clip gradient :  tensor(2.7448)\n",
            "54.30173110961914\n",
            "Clip gradient :  tensor(4.1810)\n",
            "37.49923324584961\n",
            "Clip gradient :  tensor(3.4313)\n",
            "24.63099479675293\n",
            "Clip gradient :  tensor(2.3857)\n",
            "18.092092514038086\n",
            "Clip gradient :  tensor(1.7644)\n",
            "13.964195251464844\n",
            "Clip gradient :  tensor(1.6186)\n",
            "11.252017974853516\n",
            "Clip gradient :  tensor(1.9620)\n",
            "10.3819580078125\n",
            "Clip gradient :  tensor(27.7275)\n",
            "10.849030494689941\n",
            "Clip gradient :  tensor(18.5046)\n",
            "10.75693130493164\n",
            "Clip gradient :  tensor(5.6473)\n",
            "9.422608375549316\n",
            "Clip gradient :  tensor(6.0301)\n",
            "10.918243408203125\n",
            "Clip gradient :  tensor(8.4464)\n",
            "12.61240291595459\n",
            "Clip gradient :  tensor(15.1382)\n",
            "10.411482810974121\n",
            "Clip gradient :  tensor(10.3801)\n",
            "11.736542701721191\n",
            "Clip gradient :  tensor(8.5519)\n",
            "11.328282356262207\n",
            "Clip gradient :  tensor(15.1890)\n",
            "12.937443733215332\n",
            "Clip gradient :  tensor(8.6675)\n",
            "11.043636322021484\n",
            "Clip gradient :  tensor(10.7257)\n",
            "9.922435760498047\n",
            "Clip gradient :  tensor(8.3808)\n",
            "12.866291046142578\n",
            "Clip gradient :  tensor(12.9934)\n",
            "10.71718978881836\n",
            "Clip gradient :  tensor(9.3591)\n",
            "11.553709030151367\n",
            "Clip gradient :  tensor(9.0110)\n",
            "11.449419975280762\n",
            "Clip gradient :  tensor(10.9611)\n",
            "11.62049674987793\n",
            "Clip gradient :  tensor(20.8670)\n",
            "8.340048789978027\n",
            "Clip gradient :  tensor(7.3649)\n",
            "7.723600387573242\n",
            "Clip gradient :  tensor(11.5648)\n",
            "7.478190898895264\n",
            "Clip gradient :  tensor(8.1978)\n",
            "8.2199068069458\n",
            "Clip gradient :  tensor(14.0312)\n",
            "8.603954315185547\n",
            "Clip gradient :  tensor(9.5581)\n",
            "8.177401542663574\n",
            "Clip gradient :  tensor(11.1355)\n",
            "7.209262847900391\n",
            "Clip gradient :  tensor(8.5981)\n",
            "7.455480098724365\n",
            "Clip gradient :  tensor(10.2477)\n",
            "6.475743770599365\n",
            "Clip gradient :  tensor(8.6614)\n",
            "6.504319190979004\n",
            "Clip gradient :  tensor(7.7236)\n",
            "6.817241191864014\n",
            "Clip gradient :  tensor(8.1616)\n",
            "6.581027030944824\n",
            "Clip gradient :  tensor(10.2530)\n",
            "5.909099578857422\n",
            "Clip gradient :  tensor(5.6697)\n",
            "8.019516944885254\n",
            "Clip gradient :  tensor(26.9319)\n",
            "6.5540900230407715\n",
            "Clip gradient :  tensor(7.7782)\n",
            "5.739179611206055\n",
            "Clip gradient :  tensor(4.2670)\n",
            "5.150533199310303\n",
            "Clip gradient :  tensor(3.3787)\n",
            "5.488378047943115\n",
            "Clip gradient :  tensor(5.1122)\n",
            "6.681076526641846\n",
            "Clip gradient :  tensor(6.8267)\n",
            "6.286208152770996\n",
            "Clip gradient :  tensor(11.8087)\n",
            "5.504680156707764\n",
            "Clip gradient :  tensor(6.6926)\n",
            "9.229055404663086\n",
            "Clip gradient :  tensor(18.9439)\n",
            "7.116538047790527\n",
            "Clip gradient :  tensor(12.0976)\n",
            "5.843059539794922\n",
            "Clip gradient :  tensor(15.5403)\n",
            "4.724583148956299\n",
            "Clip gradient :  tensor(3.6993)\n",
            "4.2893853187561035\n",
            "Clip gradient :  tensor(3.7930)\n",
            "4.08223819732666\n",
            "Clip gradient :  tensor(3.4849)\n",
            "5.160815238952637\n",
            "Clip gradient :  tensor(6.0946)\n",
            "6.551382541656494\n",
            "Clip gradient :  tensor(16.0383)\n",
            "5.893846035003662\n",
            "Clip gradient :  tensor(7.6988)\n",
            "7.371321678161621\n",
            "Clip gradient :  tensor(15.2729)\n",
            "5.109972953796387\n",
            "Clip gradient :  tensor(9.9375)\n",
            "4.223565578460693\n",
            "Clip gradient :  tensor(5.9050)\n",
            "4.875583648681641\n",
            "Clip gradient :  tensor(8.2955)\n",
            "4.480869293212891\n",
            "Clip gradient :  tensor(6.3235)\n",
            "4.60286283493042\n",
            "Clip gradient :  tensor(4.4700)\n",
            "6.933197498321533\n",
            "Clip gradient :  tensor(11.3355)\n",
            "5.757198810577393\n",
            "Clip gradient :  tensor(6.8505)\n",
            "5.36590051651001\n",
            "Clip gradient :  tensor(9.2380)\n",
            "4.738742351531982\n",
            "Clip gradient :  tensor(6.3986)\n",
            "3.8926403522491455\n",
            "Clip gradient :  tensor(3.5134)\n",
            "3.446578025817871\n",
            "Clip gradient :  tensor(2.0743)\n",
            "3.676034450531006\n",
            "Clip gradient :  tensor(16.1203)\n",
            "7.229807376861572\n",
            "Clip gradient :  tensor(6.7197)\n",
            "6.472393035888672\n",
            "Clip gradient :  tensor(7.1655)\n",
            "5.925787448883057\n",
            "Clip gradient :  tensor(6.7619)\n",
            "4.319212913513184\n",
            "Clip gradient :  tensor(7.0963)\n",
            "3.6114895343780518\n",
            "Clip gradient :  tensor(8.6701)\n",
            "3.1534013748168945\n",
            "Clip gradient :  tensor(2.2136)\n",
            "3.145663261413574\n",
            "Clip gradient :  tensor(4.8356)\n",
            "3.7090835571289062\n",
            "Clip gradient :  tensor(4.5655)\n",
            "3.5856258869171143\n",
            "Clip gradient :  tensor(3.4741)\n",
            "3.120832681655884\n",
            "Clip gradient :  tensor(1.7359)\n",
            "2.9244275093078613\n",
            "Clip gradient :  tensor(2.5132)\n",
            "2.8785674571990967\n",
            "Clip gradient :  tensor(1.8380)\n",
            "2.676112413406372\n",
            "Clip gradient :  tensor(0.8734)\n",
            "2.578947067260742\n",
            "Clip gradient :  tensor(0.8765)\n",
            "2.5079195499420166\n",
            "Clip gradient :  tensor(1.7874)\n",
            "2.4388554096221924\n",
            "Clip gradient :  tensor(0.8288)\n",
            "2.3852620124816895\n",
            "Clip gradient :  tensor(1.3594)\n",
            "2.304316759109497\n",
            "Clip gradient :  tensor(0.5613)\n",
            "2.2472574710845947\n",
            "Clip gradient :  tensor(0.5735)\n",
            "2.498654365539551\n",
            "Clip gradient :  tensor(5.9545)\n",
            "3.98103666305542\n",
            "Clip gradient :  tensor(17.5775)\n",
            "5.114433765411377\n",
            "Clip gradient :  tensor(9.4073)\n",
            "7.835857391357422\n",
            "Clip gradient :  tensor(19.2812)\n",
            "5.390251159667969\n",
            "Clip gradient :  tensor(2.3928)\n",
            "3.45634388923645\n",
            "Clip gradient :  tensor(3.2554)\n",
            "2.6984665393829346\n",
            "Clip gradient :  tensor(2.9525)\n",
            "2.3903656005859375\n",
            "Clip gradient :  tensor(2.4108)\n",
            "2.2600882053375244\n",
            "Clip gradient :  tensor(1.6576)\n",
            "2.160277843475342\n",
            "Clip gradient :  tensor(0.4141)\n",
            "2.1131668090820312\n",
            "Clip gradient :  tensor(0.4566)\n",
            "2.066528797149658\n",
            "Clip gradient :  tensor(0.2541)\n",
            "2.0287256240844727\n",
            "Clip gradient :  tensor(0.2372)\n"
          ]
        }
      ],
      "source": [
        "CLIP_GRAD = True\n",
        "\n",
        "for epoch in range(e_cnt):\n",
        "    hh = torch.zeros(3)\n",
        "    loss = 0\n",
        "    optim.zero_grad()\n",
        "    for sample, next_sample in ds:\n",
        "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
        "\n",
        "        target =  torch.LongTensor([next_sample])\n",
        "\n",
        "        y, hh = rnn(x, hh)\n",
        "\n",
        "        loss += criterion(y.unsqueeze(0), target)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print (loss.data.item())\n",
        "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
        "    else:\n",
        "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
        "\n",
        "#     print(\"Params : \")\n",
        "#     num_params = 0\n",
        "#     for item in rnn.parameters():\n",
        "#         num_params += 1\n",
        "#         print(item.grad)\n",
        "#     print(\"NumParams :\", num_params)\n",
        "#     print(\"Optimize\")\n",
        "\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfRDM3N2d4Kb"
      },
      "source": [
        "# Тестирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FVuchBxd4Kb",
        "outputId": "99777e4d-2157-4611-acc6-24fe8bbe64be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:\t ololoasdasddqweqw123456789\n",
            "Original:\t ololoasdasddqweqw123456789\n"
          ]
        }
      ],
      "source": [
        "rnn.eval()\n",
        "hh = torch.zeros(3)\n",
        "id = 0\n",
        "softmax  = nn.Softmax(dim=1)\n",
        "predword = ds.get_char_by_id(id)\n",
        "for c in enumerate(word[:-1]):\n",
        "    x = ds.get_one_hot(id).unsqueeze(0)\n",
        "    y, hh = rnn(x, hh)\n",
        "    y = softmax(y.unsqueeze(0))\n",
        "    m, id = torch.max(y, 1)\n",
        "    id = id.data[0]\n",
        "    predword += ds.get_char_by_id(id)\n",
        "print ('Prediction:\\t' , predword)\n",
        "print(\"Original:\\t\", word)\n",
        "assert(predword == word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qfLI1AYd4Kb"
      },
      "source": [
        "# ДЗ\n",
        "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "FZaQQticd4Kc"
      },
      "outputs": [],
      "source": [
        "#тестовое слово\n",
        "word = 'ololoasdasddqweqw123456789asdfzxcv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GOLWqld4Kc"
      },
      "source": [
        "## Реализовать LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "cSYduULbd4Kc"
      },
      "outputs": [],
      "source": [
        "#Написать реализацию LSTM и обучить предсказывать слово\n",
        "\n",
        "class VanillaLSTM(nn.Module):\n",
        "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
        "        super(VanillaLSTM, self).__init__()\n",
        "        self.hidden_ft = nn.Linear(in_features=in_size + hidden_size, out_features=hidden_size)\n",
        "        self.hidden_it = nn.Linear(in_features=in_size + hidden_size, out_features=hidden_size)\n",
        "        self.hidden_ct = nn.Linear(in_features=in_size + hidden_size, out_features=hidden_size)\n",
        "        self.hidden_ot = nn.Linear(in_features=in_size + hidden_size, out_features=hidden_size)\n",
        "        self.hidden_ct_ot = nn.Linear(in_features=out_size, out_features=hidden_size)\n",
        "        self.activation_sigmoid = nn.Sigmoid()\n",
        "        self.activation_tanh = nn.Tanh()\n",
        "        self.out = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
        "\n",
        "    def forward(self, prev_c, x, prev_hidden):\n",
        "        hidden = torch.cat([x.squeeze(0), prev_hidden])\n",
        "        hidden_ft = self.hidden_ft(hidden)\n",
        "        hidden_ft = self.activation_sigmoid(hidden_ft)\n",
        "\n",
        "        hidden_it = self.hidden_it(hidden)\n",
        "        hidden_it = self.activation_sigmoid(hidden_it)\n",
        "\n",
        "        hidden_ct = self.hidden_ct(hidden)\n",
        "        hidden_ct = self.activation_tanh(hidden_ct)\n",
        "\n",
        "        hidden_ot = self.hidden_ot(hidden)\n",
        "        hidden_ot = self.activation_sigmoid(hidden_ot)\n",
        "\n",
        "\n",
        "        next_ct = prev_c * hidden_ft + hidden_it * hidden_ct\n",
        "        next_hidden = hidden_ot * self.activation_tanh(next_ct)\n",
        "        output = self.out(next_hidden)\n",
        "        return output, next_hidden, next_ct\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = WordDataSet(word=word)\n",
        "rnn = VanillaLSTM(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "e_cnt = 1000\n",
        "optim = SGD(rnn.parameters(), lr = 0.05, momentum=0.9)"
      ],
      "metadata": {
        "id": "afFYMhuns3gS"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP_GRAD = True\n",
        "\n",
        "for epoch in range(e_cnt):\n",
        "    hh = torch.zeros(3)\n",
        "    c = torch.zeros(3)\n",
        "    loss = 0\n",
        "    optim.zero_grad()\n",
        "    for sample, next_sample in ds:\n",
        "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
        "\n",
        "        target = torch.LongTensor([next_sample])\n",
        "\n",
        "        y, hh, c = rnn(c, x, hh)\n",
        "\n",
        "        loss += criterion(y.unsqueeze(0), target)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print (loss.data.item())\n",
        "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
        "    else:\n",
        "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
        "\n",
        "#     print(\"Params : \")\n",
        "#     num_params = 0\n",
        "#     for item in rnn.parameters():\n",
        "#         num_params += 1\n",
        "#         print(item.grad)\n",
        "#     print(\"NumParams :\", num_params)\n",
        "#     print(\"Optimize\")\n",
        "\n",
        "    optim.step()"
      ],
      "metadata": {
        "id": "SNrK18ESlWPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e7099e-2803-4b85-ec8c-335a7cbeed2b"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104.06502532958984\n",
            "Clip gradient :  tensor(5.0829)\n",
            "97.10514068603516\n",
            "Clip gradient :  tensor(2.1895)\n",
            "92.19776916503906\n",
            "Clip gradient :  tensor(3.3306)\n",
            "77.46189880371094\n",
            "Clip gradient :  tensor(5.8421)\n",
            "63.919002532958984\n",
            "Clip gradient :  tensor(8.1943)\n",
            "56.250770568847656\n",
            "Clip gradient :  tensor(10.2773)\n",
            "49.300968170166016\n",
            "Clip gradient :  tensor(5.9047)\n",
            "43.318603515625\n",
            "Clip gradient :  tensor(5.3784)\n",
            "40.756797790527344\n",
            "Clip gradient :  tensor(21.3137)\n",
            "37.9990348815918\n",
            "Clip gradient :  tensor(9.1275)\n",
            "38.40715789794922\n",
            "Clip gradient :  tensor(14.0208)\n",
            "36.11970520019531\n",
            "Clip gradient :  tensor(7.3123)\n",
            "33.069664001464844\n",
            "Clip gradient :  tensor(5.3003)\n",
            "31.058551788330078\n",
            "Clip gradient :  tensor(5.3910)\n",
            "28.899682998657227\n",
            "Clip gradient :  tensor(5.7410)\n",
            "27.382781982421875\n",
            "Clip gradient :  tensor(5.3038)\n",
            "25.873655319213867\n",
            "Clip gradient :  tensor(5.8462)\n",
            "23.898359298706055\n",
            "Clip gradient :  tensor(3.6185)\n",
            "21.9276123046875\n",
            "Clip gradient :  tensor(2.6701)\n",
            "20.011856079101562\n",
            "Clip gradient :  tensor(2.4386)\n",
            "18.61040687561035\n",
            "Clip gradient :  tensor(2.3662)\n",
            "17.304866790771484\n",
            "Clip gradient :  tensor(2.8394)\n",
            "16.739835739135742\n",
            "Clip gradient :  tensor(4.9343)\n",
            "17.303218841552734\n",
            "Clip gradient :  tensor(13.5712)\n",
            "16.862276077270508\n",
            "Clip gradient :  tensor(7.1901)\n",
            "15.550898551940918\n",
            "Clip gradient :  tensor(4.9767)\n",
            "15.018973350524902\n",
            "Clip gradient :  tensor(4.7805)\n",
            "15.357226371765137\n",
            "Clip gradient :  tensor(6.8904)\n",
            "15.639067649841309\n",
            "Clip gradient :  tensor(7.5604)\n",
            "15.957718849182129\n",
            "Clip gradient :  tensor(7.9702)\n",
            "14.820622444152832\n",
            "Clip gradient :  tensor(7.5105)\n",
            "15.440937042236328\n",
            "Clip gradient :  tensor(10.4606)\n",
            "13.997078895568848\n",
            "Clip gradient :  tensor(10.5473)\n",
            "13.814130783081055\n",
            "Clip gradient :  tensor(6.4788)\n",
            "15.593082427978516\n",
            "Clip gradient :  tensor(12.3732)\n",
            "14.451090812683105\n",
            "Clip gradient :  tensor(9.2101)\n",
            "14.603775978088379\n",
            "Clip gradient :  tensor(9.7966)\n",
            "13.378808975219727\n",
            "Clip gradient :  tensor(8.1756)\n",
            "13.018841743469238\n",
            "Clip gradient :  tensor(7.0479)\n",
            "13.055542945861816\n",
            "Clip gradient :  tensor(9.7348)\n",
            "12.365546226501465\n",
            "Clip gradient :  tensor(7.1001)\n",
            "12.081643104553223\n",
            "Clip gradient :  tensor(6.8975)\n",
            "12.07266902923584\n",
            "Clip gradient :  tensor(6.5163)\n",
            "12.356568336486816\n",
            "Clip gradient :  tensor(8.3223)\n",
            "11.973564147949219\n",
            "Clip gradient :  tensor(7.0539)\n",
            "11.104461669921875\n",
            "Clip gradient :  tensor(6.7928)\n",
            "10.496176719665527\n",
            "Clip gradient :  tensor(5.4566)\n",
            "9.982464790344238\n",
            "Clip gradient :  tensor(5.4628)\n",
            "10.222163200378418\n",
            "Clip gradient :  tensor(6.1503)\n",
            "9.591856002807617\n",
            "Clip gradient :  tensor(5.5305)\n",
            "9.467479705810547\n",
            "Clip gradient :  tensor(5.7461)\n",
            "9.011826515197754\n",
            "Clip gradient :  tensor(5.4566)\n",
            "8.623848915100098\n",
            "Clip gradient :  tensor(5.1940)\n",
            "8.208395004272461\n",
            "Clip gradient :  tensor(4.8553)\n",
            "7.866523265838623\n",
            "Clip gradient :  tensor(5.1782)\n",
            "7.575714588165283\n",
            "Clip gradient :  tensor(4.2508)\n",
            "7.0698933601379395\n",
            "Clip gradient :  tensor(3.7273)\n",
            "6.765284538269043\n",
            "Clip gradient :  tensor(3.7359)\n",
            "6.631592750549316\n",
            "Clip gradient :  tensor(4.2365)\n",
            "6.4759087562561035\n",
            "Clip gradient :  tensor(4.4601)\n",
            "6.231354713439941\n",
            "Clip gradient :  tensor(4.0054)\n",
            "6.050522804260254\n",
            "Clip gradient :  tensor(4.0954)\n",
            "5.874753952026367\n",
            "Clip gradient :  tensor(4.0303)\n",
            "5.732105731964111\n",
            "Clip gradient :  tensor(4.4586)\n",
            "5.57328987121582\n",
            "Clip gradient :  tensor(3.9624)\n",
            "5.3592987060546875\n",
            "Clip gradient :  tensor(3.7663)\n",
            "5.157943248748779\n",
            "Clip gradient :  tensor(3.8826)\n",
            "4.983461380004883\n",
            "Clip gradient :  tensor(3.4592)\n",
            "4.823758602142334\n",
            "Clip gradient :  tensor(4.2914)\n",
            "4.736177921295166\n",
            "Clip gradient :  tensor(4.0829)\n",
            "4.351923942565918\n",
            "Clip gradient :  tensor(3.3771)\n",
            "4.232428550720215\n",
            "Clip gradient :  tensor(3.5043)\n",
            "4.072028636932373\n",
            "Clip gradient :  tensor(3.7095)\n",
            "4.248741626739502\n",
            "Clip gradient :  tensor(4.9285)\n",
            "4.269266605377197\n",
            "Clip gradient :  tensor(3.6469)\n",
            "4.244593143463135\n",
            "Clip gradient :  tensor(5.2277)\n",
            "4.07354736328125\n",
            "Clip gradient :  tensor(6.0229)\n",
            "3.8895514011383057\n",
            "Clip gradient :  tensor(2.6289)\n",
            "3.52543306350708\n",
            "Clip gradient :  tensor(1.5872)\n",
            "3.2838127613067627\n",
            "Clip gradient :  tensor(1.3774)\n",
            "3.037740468978882\n",
            "Clip gradient :  tensor(0.5579)\n",
            "2.823244571685791\n",
            "Clip gradient :  tensor(0.4192)\n",
            "2.6347243785858154\n",
            "Clip gradient :  tensor(0.3045)\n",
            "2.470978260040283\n",
            "Clip gradient :  tensor(0.2351)\n",
            "2.327003002166748\n",
            "Clip gradient :  tensor(0.1898)\n",
            "2.199061155319214\n",
            "Clip gradient :  tensor(0.1580)\n",
            "2.0844674110412598\n",
            "Clip gradient :  tensor(0.1491)\n",
            "1.9811280965805054\n",
            "Clip gradient :  tensor(0.1391)\n",
            "1.8874491453170776\n",
            "Clip gradient :  tensor(0.1309)\n",
            "1.8021069765090942\n",
            "Clip gradient :  tensor(0.1252)\n",
            "1.7240173816680908\n",
            "Clip gradient :  tensor(0.1206)\n",
            "1.6548614501953125\n",
            "Clip gradient :  tensor(0.6733)\n",
            "1.6252264976501465\n",
            "Clip gradient :  tensor(2.2713)\n",
            "1.5530424118041992\n",
            "Clip gradient :  tensor(1.1487)\n",
            "1.5452653169631958\n",
            "Clip gradient :  tensor(2.2564)\n",
            "1.7545086145401\n",
            "Clip gradient :  tensor(8.3593)\n",
            "3.6172313690185547\n",
            "Clip gradient :  tensor(36.3608)\n",
            "2.0264177322387695\n",
            "Clip gradient :  tensor(7.6065)\n",
            "1.8105441331863403\n",
            "Clip gradient :  tensor(6.6478)\n",
            "1.6018075942993164\n",
            "Clip gradient :  tensor(1.1750)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.eval()\n",
        "hh = torch.zeros(3)\n",
        "c = torch.zeros(3)\n",
        "id = 0\n",
        "softmax  = nn.Softmax(dim=1)\n",
        "predword = ds.get_char_by_id(id)\n",
        "for idx in enumerate(word[:-1]):\n",
        "    x = ds.get_one_hot(id).unsqueeze(0)\n",
        "    y, hh, c = rnn(c, x, hh)\n",
        "    y = softmax(y.unsqueeze(0))\n",
        "    m, id = torch.max(y, 1)\n",
        "    id = id.data[0]\n",
        "    predword += ds.get_char_by_id(id)\n",
        "print ('Prediction:\\t' , predword)\n",
        "print(\"Original:\\t\", word)\n",
        "assert(predword == word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-KseWdF1Sqz",
        "outputId": "c5df219f-9241-438f-85a6-a0547530ed1e"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:\t ololoasdasddqweqw123456789asdfzxcv\n",
            "Original:\t ololoasdasddqweqw123456789asdfzxcv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw9mURPjd4Kc"
      },
      "source": [
        "## Реализовать GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrjCkIzNd4Kc"
      },
      "outputs": [],
      "source": [
        "#Написать реализацию GRU и обучить предсказывать слово\n",
        "class VanillaGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
        "        super(VanillaGRU, self).__init__()\n",
        "        self.hidden_rt = nn.Linear(in_features=in_size + hidden_size, out_features=hidden_size)\n",
        "        self.hidden_zt = nn.Linear(in_features=in_size + hidden_size, out_features=hidden_size)\n",
        "        self.hidden_ht = nn.Linear(in_features=in_size + hidden_size, out_features=hidden_size)\n",
        "        self.hidden_activation = nn.Sigmoid()\n",
        "        self.activation_1  = nn.Tanh()\n",
        "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_hidden):\n",
        "        hidden = torch.cat([x.squeeze(0), prev_hidden])\n",
        "        hidden_rt = self.hidden_rt(hidden)\n",
        "        hidden_rt = self.hidden_activation(hidden_rt)\n",
        "\n",
        "        hidden_zt = self.hidden_zt(hidden)\n",
        "        hidden_zt = self.hidden_activation(hidden_zt)\n",
        "\n",
        "        hidden_ht = prev_hidden * hidden_rt\n",
        "        hidden_ht = torch.cat([x.squeeze(0), hidden_ht])\n",
        "        hidden_ht = self.hidden_ht(hidden_ht)\n",
        "        hidden_ht = self.activation_1(hidden_ht)\n",
        "\n",
        "        new_hidden = prev_hidden * (1 - hidden_zt)\n",
        "        new_hidden += hidden_zt * hidden_ht\n",
        "\n",
        "        output = self.outweight(new_hidden)\n",
        "        return output, new_hidden"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}