{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8133867,"sourceType":"datasetVersion","datasetId":4807935}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Задание: обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers. Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n\nНапишите класс для Dataset/Dataloder и азбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\nПопробуйте несколько моделей: Bert, Albert, Deberta. (3 балла) Пример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T13:33:27.754070Z","iopub.execute_input":"2024-04-22T13:33:27.754743Z","iopub.status.idle":"2024-04-22T13:33:57.443810Z","shell.execute_reply.started":"2024-04-22T13:33:27.754715Z","shell.execute_reply":"2024-04-22T13:33:57.442834Z"}}},{"cell_type":"code","source":"!pip install transformers\n!pip install lightning\n!git clone https://github.com/KuzmaKhrabrov/character-tokenizer.git","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:27:20.838121Z","iopub.execute_input":"2024-04-25T15:27:20.838476Z","iopub.status.idle":"2024-04-25T15:27:52.177382Z","shell.execute_reply.started":"2024-04-25T15:27:20.838447Z","shell.execute_reply":"2024-04-25T15:27:52.176273Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nCollecting lightning\n  Downloading lightning-2.2.3-py3-none-any.whl.metadata (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.2.3-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.3\nCloning into 'character-tokenizer'...\nremote: Enumerating objects: 20, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 20 (delta 5), reused 10 (delta 3), pack-reused 0\u001b[K\nUnpacking objects: 100% (20/20), 5.87 KiB | 1.17 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport string\nimport sys\n\nsys.path.append(\"/kaggle/working/character-tokenizer\")\nfrom charactertokenizer import CharacterTokenizer\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nimport wandb\n\nimport lightning.pytorch as pl\nfrom lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom lightning.pytorch.callbacks import LearningRateMonitor\nfrom lightning.pytorch.loggers import WandbLogger\n\n\nfrom transformers import AutoTokenizer\nfrom transformers.tokenization_utils import PreTrainedTokenizer\n\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:27:52.179375Z","iopub.execute_input":"2024-04-25T15:27:52.179760Z","iopub.status.idle":"2024-04-25T15:28:03.348238Z","shell.execute_reply.started":"2024-04-25T15:27:52.179727Z","shell.execute_reply":"2024-04-25T15:28:03.347222Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:03.349519Z","iopub.execute_input":"2024-04-25T15:28:03.350036Z","iopub.status.idle":"2024-04-25T15:28:08.209169Z","shell.execute_reply.started":"2024-04-25T15:28:03.350006Z","shell.execute_reply":"2024-04-25T15:28:08.208403Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/all-accents/all_accents.tsv\", sep='\\t', names=['original', 'accented'])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:09.947857Z","iopub.execute_input":"2024-04-25T15:28:09.948891Z","iopub.status.idle":"2024-04-25T15:28:13.763662Z","shell.execute_reply.started":"2024-04-25T15:28:09.948856Z","shell.execute_reply":"2024-04-25T15:28:13.762795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-25T14:48:57.391720Z","iopub.execute_input":"2024-04-25T14:48:57.392656Z","iopub.status.idle":"2024-04-25T14:48:57.408730Z","shell.execute_reply.started":"2024-04-25T14:48:57.392627Z","shell.execute_reply":"2024-04-25T14:48:57.407787Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                original          accented\n0                    -де              -д^е\n1                    -ка              -к^а\n2                  -либо            -л^ибо\n3                -нибудь          -ниб^удь\n4                     -с                -с\n...                  ...               ...\n1680530  ӂюль-верновский  ӂюль-в^ерновский\n1680531           ӂюрить           ӂюр^ить\n1680532           ӂӂение           ӂӂ^ение\n1680533          ӂӂенный           ӂӂенный\n1680534           ӂӂеный            ӂӂеный\n\n[1680535 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original</th>\n      <th>accented</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-де</td>\n      <td>-д^е</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-ка</td>\n      <td>-к^а</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-либо</td>\n      <td>-л^ибо</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-нибудь</td>\n      <td>-ниб^удь</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-с</td>\n      <td>-с</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1680530</th>\n      <td>ӂюль-верновский</td>\n      <td>ӂюль-в^ерновский</td>\n    </tr>\n    <tr>\n      <th>1680531</th>\n      <td>ӂюрить</td>\n      <td>ӂюр^ить</td>\n    </tr>\n    <tr>\n      <th>1680532</th>\n      <td>ӂӂение</td>\n      <td>ӂӂ^ение</td>\n    </tr>\n    <tr>\n      <th>1680533</th>\n      <td>ӂӂенный</td>\n      <td>ӂӂенный</td>\n    </tr>\n    <tr>\n      <th>1680534</th>\n      <td>ӂӂеный</td>\n      <td>ӂӂеный</td>\n    </tr>\n  </tbody>\n</table>\n<p>1680535 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# example = \"Привет\"\n# tokens = tokenizer(example)\n# print(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:22:59.281862Z","iopub.execute_input":"2024-04-22T14:22:59.282148Z","iopub.status.idle":"2024-04-22T14:22:59.286128Z","shell.execute_reply.started":"2024-04-22T14:22:59.282125Z","shell.execute_reply":"2024-04-22T14:22:59.285261Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class AccentDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df.reset_index()\n        self.max_length = max_length\n        self.df = self.df.drop(columns=['index'])\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self, idx):\n        orig = self.df.loc[idx]['original']\n        accent_id = self.df.loc[idx]['accented'].find('^')\n\n        tokenized_item = self.tokenizer(orig,\n                                        max_length=self.max_length,\n                                        padding='max_length',\n                                        truncation=True,\n                                        return_tensors='pt')\n        \n        del tokenized_item['token_type_ids']\n        \n        tokenized_item['input_ids'] = tokenized_item['input_ids'].flatten()\n        tokenized_item['attention_mask'] = tokenized_item['attention_mask'].flatten()\n        \n        tokenized_item['labels'] = torch.zeros_like(tokenized_item['input_ids'].flatten())\n        tokenized_item['labels'][accent_id + 1] = 1\n#         tokenized_item['raw_word'] = torch.String(orig)\n        \n        return tokenized_item\n    \n    def __len__(self):\n        return len(self.df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:24.491257Z","iopub.execute_input":"2024-04-25T15:28:24.491639Z","iopub.status.idle":"2024-04-25T15:28:24.500702Z","shell.execute_reply.started":"2024-04-25T15:28:24.491608Z","shell.execute_reply":"2024-04-25T15:28:24.499658Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.head(len(df) // 2)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:43:45.109123Z","iopub.execute_input":"2024-04-23T19:43:45.109511Z","iopub.status.idle":"2024-04-23T19:43:45.128955Z","shell.execute_reply.started":"2024-04-23T19:43:45.109483Z","shell.execute_reply":"2024-04-23T19:43:45.127808Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"              original         accented\n0                  -де             -д^е\n1                  -ка             -к^а\n2                -либо           -л^ибо\n3              -нибудь         -ниб^удь\n4                   -с               -с\n...                ...              ...\n840262     объективное     объект^ивное\n840263     объективной     объект^ивной\n840264     объективном     объект^ивном\n840265    объективному    объект^ивному\n840266  объективностей  объект^ивностей\n\n[840267 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original</th>\n      <th>accented</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-де</td>\n      <td>-д^е</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-ка</td>\n      <td>-к^а</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-либо</td>\n      <td>-л^ибо</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-нибудь</td>\n      <td>-ниб^удь</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-с</td>\n      <td>-с</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>840262</th>\n      <td>объективное</td>\n      <td>объект^ивное</td>\n    </tr>\n    <tr>\n      <th>840263</th>\n      <td>объективной</td>\n      <td>объект^ивной</td>\n    </tr>\n    <tr>\n      <th>840264</th>\n      <td>объективном</td>\n      <td>объект^ивном</td>\n    </tr>\n    <tr>\n      <th>840265</th>\n      <td>объективному</td>\n      <td>объект^ивному</td>\n    </tr>\n    <tr>\n      <th>840266</th>\n      <td>объективностей</td>\n      <td>объект^ивностей</td>\n    </tr>\n  </tbody>\n</table>\n<p>840267 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model_max_length = 256\nbatch_size = 256\nchars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\ntokenizer = CharacterTokenizer(chars, model_max_length)\n\n# df = df.sample(frac=1)\ndf = df.head(len(df) // 5)\n\ntrain_df, val_df = train_test_split(df, test_size=0.5)\n\ntrain_dataset = AccentDataset(train_df, tokenizer, model_max_length)\nval_dataset = AccentDataset(val_df.iloc[:len(val_df)//2, :], tokenizer, model_max_length)\ntest_dataset = AccentDataset(val_df.iloc[len(val_df)//2:, :], tokenizer, model_max_length)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=3)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=3)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:26.523162Z","iopub.execute_input":"2024-04-25T15:28:26.523538Z","iopub.status.idle":"2024-04-25T15:28:26.565617Z","shell.execute_reply.started":"2024-04-25T15:28:26.523509Z","shell.execute_reply":"2024-04-25T15:28:26.564593Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"val_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T21:18:03.564966Z","iopub.execute_input":"2024-04-23T21:18:03.565636Z","iopub.status.idle":"2024-04-23T21:18:03.582266Z","shell.execute_reply.started":"2024-04-23T21:18:03.565595Z","shell.execute_reply":"2024-04-23T21:18:03.581278Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([ 0, 12, 64, 42, 38, 36, 18, 36, 36, 38, 14, 38,  1,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"},"metadata":{}}]},{"cell_type":"code","source":"next(iter(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:22:59.959705Z","iopub.execute_input":"2024-04-22T14:22:59.959973Z","iopub.status.idle":"2024-04-22T14:23:00.577182Z","shell.execute_reply.started":"2024-04-22T14:22:59.959950Z","shell.execute_reply":"2024-04-22T14:23:00.576133Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 0, 36,  8,  ...,  4,  4,  4],\n        [ 0, 30, 38,  ...,  4,  4,  4],\n        [ 0, 34,  8,  ...,  4,  4,  4],\n        ...,\n        [ 0, 40, 18,  ...,  4,  4,  4],\n        [ 0, 16, 38,  ...,  4,  4,  4],\n        [ 0, 40, 42,  ...,  4,  4,  4]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DebertaV2Config, DebertaV2ForTokenClassification\n\ndeberta_config = DebertaV2Config(hidden_size=256,\n                        intermediate_size=1024,\n                        max_length=40,\n                        model_type='deberta-v2',\n                        num_attention_heads=8,\n                        num_hidden_layers=4,\n                        position_biased_input=True,\n                        relative_attention=True)\n\ndeberta_model = DebertaV2ForTokenClassification(config)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T10:15:17.447703Z","iopub.execute_input":"2024-04-24T10:15:17.448403Z","iopub.status.idle":"2024-04-24T10:15:19.234129Z","shell.execute_reply.started":"2024-04-24T10:15:17.448371Z","shell.execute_reply":"2024-04-24T10:15:19.233096Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:23:02.281220Z","iopub.execute_input":"2024-04-22T14:23:02.281693Z","iopub.status.idle":"2024-04-22T14:23:02.288435Z","shell.execute_reply.started":"2024-04-22T14:23:02.281660Z","shell.execute_reply":"2024-04-22T14:23:02.287594Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DebertaV2ForTokenClassification(\n  (deberta): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 256, padding_idx=0)\n      (position_embeddings): Embedding(512, 256)\n      (LayerNorm): LayerNorm((256,), eps=1e-07, elementwise_affine=True)\n      (dropout): StableDropout()\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-3): 4 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=256, out_features=256, bias=True)\n              (key_proj): Linear(in_features=256, out_features=256, bias=True)\n              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n              (pos_dropout): StableDropout()\n              (dropout): StableDropout()\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 256)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=256, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"test_data = next(iter(train_dataloader))\nmodel_output = model(test_data['input_ids'], attention_mask=test_data['attention_mask'], labels=test_data['labels'])\nprint(model_output.logits)\nprint(model_output.logits.size())\nprint(test_data['input_ids'].size())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:23:02.289708Z","iopub.execute_input":"2024-04-22T14:23:02.289975Z","iopub.status.idle":"2024-04-22T14:23:20.263787Z","shell.execute_reply.started":"2024-04-22T14:23:02.289952Z","shell.execute_reply":"2024-04-22T14:23:20.262660Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"tensor([[[ 0.0036,  0.5507],\n         [-0.2701, -0.3176],\n         [-0.1553, -0.4632],\n         ...,\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000]],\n\n        [[-0.3081,  0.2325],\n         [ 0.0426, -0.4880],\n         [ 0.1171, -0.2132],\n         ...,\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000]],\n\n        [[-0.2241,  0.5807],\n         [-0.2065, -0.0069],\n         [-0.5547,  0.3684],\n         ...,\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000]],\n\n        ...,\n\n        [[-0.0894,  0.6162],\n         [-0.1279, -0.0971],\n         [-0.2654, -0.1469],\n         ...,\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000]],\n\n        [[-0.3648,  0.4931],\n         [-0.4191, -0.2587],\n         [-0.0527, -0.1440],\n         ...,\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000]],\n\n        [[-0.1769,  0.5233],\n         [ 0.1900, -0.4491],\n         [ 0.0568,  0.4458],\n         ...,\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000],\n         [ 0.0000,  0.0000]]], grad_fn=<ViewBackward0>)\ntorch.Size([256, 256, 2])\ntorch.Size([256, 256])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.argmax(model_output.logits[:, :, 1], dim=1))\ntorch.argmax(test_data['labels'], dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:23:20.265404Z","iopub.execute_input":"2024-04-22T14:23:20.265891Z","iopub.status.idle":"2024-04-22T14:23:20.279478Z","shell.execute_reply.started":"2024-04-22T14:23:20.265853Z","shell.execute_reply":"2024-04-22T14:23:20.278582Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"tensor([ 7,  7, 10,  5,  7,  0,  0,  7,  5,  7,  5,  7,  7, 10,  7,  7,  0,  5,\n         5,  0,  6, 10, 11,  7, 12,  7,  7, 11,  0,  7,  5,  7,  7, 10,  7,  5,\n         7,  6,  5,  7, 11,  2, 11,  0,  0,  0, 13,  7,  7,  7, 11, 11,  7,  5,\n         7, 10, 11, 14,  7,  7,  0,  7,  7,  9,  0, 13,  0,  5,  7, 11,  7,  5,\n        11,  7,  7,  0,  0, 12,  7,  4,  0,  7, 10,  7,  2,  0,  7,  5,  0, 11,\n         7,  0, 13,  0,  7,  7,  0,  2,  5,  0, 13,  7,  5,  5,  7,  7,  7, 11,\n         7,  4,  7,  7,  7, 10,  0,  7,  0,  5,  7,  7,  7,  7,  7,  7,  6,  0,\n        13,  7,  0,  7,  5,  0,  0, 13,  0,  7, 10, 10, 13,  2,  2, 11,  7,  7,\n         1, 11,  7,  0,  7,  7, 13,  2,  0,  0,  7,  7, 10,  7,  7,  4,  7,  7,\n         5,  0,  7,  7, 11,  7,  7,  7,  0,  7,  7, 11,  4,  9,  7,  5,  0,  0,\n         5,  0,  7,  7,  0,  0,  4,  9, 10,  7,  4,  6,  7, 11,  7,  4,  7, 13,\n        13, 11,  7, 13,  5,  0,  7,  4,  7,  4,  7, 11, 21, 13,  0,  9, 10,  4,\n        10,  0,  7, 11, 11,  0,  0, 13,  0,  0,  5,  0,  7,  7,  5,  7,  9,  5,\n         7,  7,  0,  7,  7,  4, 13,  9,  5,  7,  0, 11,  0,  7, 11, 10, 11,  7,\n         2,  7,  7,  0])\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([ 8,  6,  7,  6, 10,  2,  5,  2,  6,  6, 10,  9,  8,  9,  6,  6,  5,  4,\n         0,  5,  4,  5,  9,  4,  6,  6,  4,  6,  7,  2,  6,  9,  4,  9, 10,  6,\n         4,  2,  6,  7,  7,  1,  7, 13,  8,  5, 11,  7,  6,  1,  6,  7,  8,  2,\n         5,  6,  5,  6,  6,  5,  4,  7,  4,  2,  3,  8, 10,  9,  9,  5,  4,  7,\n         7,  3, 10,  5,  7,  7,  4,  6,  6,  3,  7,  6,  5,  8,  7,  8,  6,  6,\n         2,  7,  5,  7,  3,  6,  7,  7,  7,  6, 10,  3,  3,  7,  9,  6,  8,  7,\n         8,  3,  3,  5,  2,  3,  4,  4,  8,  4,  7,  5,  6,  5,  6,  4,  5,  7,\n         7,  8, 13,  3,  3,  3,  4,  5,  5,  6,  6,  4,  5,  2,  7,  6, 13,  5,\n         7,  9,  4,  8,  6,  4,  5,  2, 10,  8,  4,  4,  6,  3, 14,  5,  5,  9,\n         5,  9,  6,  6,  8,  8,  5,  4,  3,  6,  6,  6,  7, 10,  2,  7,  2,  5,\n         4, 10,  5,  6,  2,  2,  4,  8,  6,  4,  5,  7,  5,  4,  7,  5, 11,  9,\n         4,  9,  4,  8,  6,  5,  5,  7,  4, 10,  5,  7, 12, 10, 10,  7,  5,  8,\n         3,  6,  2,  6,  7,  2,  8, 11,  4,  7,  6,  3,  6,  5,  6,  6,  4,  4,\n         8,  8,  5,  5,  4,  5,  7,  3,  5,  6,  5,  5,  4,  6,  7,  5,  7,  9,\n         6,  7,  4,  4])"},"metadata":{}}]},{"cell_type":"code","source":"test_data['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2024-04-22T14:23:20.280620Z","iopub.execute_input":"2024-04-22T14:23:20.280900Z","iopub.status.idle":"2024-04-22T14:23:20.287615Z","shell.execute_reply.started":"2024-04-22T14:23:20.280876Z","shell.execute_reply":"2024-04-22T14:23:20.286694Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"class AccentModel(pl.LightningModule):\n    def __init__(self, model, tokenizer, learning_rate):\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.learning_rate = learning_rate\n#         self.save_hyperparameters()\n    \n    def training_step(self, batch, batch_idx):\n        ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        \n        preds = self.model(ids, attention_mask=attention_mask, labels=labels)\n        loss = preds.loss\n        \n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        \n        preds = self.model(ids, attention_mask=attention_mask, labels=labels)\n        loss = preds.loss\n        \n#         print(preds)\n        \n        step_accuracy = torch.sum(torch.argmax(preds.logits[:, :, 1], dim=1) == torch.argmax(batch['labels'], dim=1)) / len(torch.argmax(batch['labels'], dim=1))\n        \n        self.log('validation_accuracy', step_accuracy)\n        self.log('validation_loss', loss)\n    \n    def forward(self, x):\n        ids = x['input_ids']\n        attention_mask = x['attention_mask']\n        labels = x['labels']\n        preds = self.model(ids, attention_mask=attention_mask, labels=labels)\n        \n        return preds.logits\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return [optimizer]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:34.430716Z","iopub.execute_input":"2024-04-25T15:28:34.431512Z","iopub.status.idle":"2024-04-25T15:28:34.442322Z","shell.execute_reply.started":"2024-04-25T15:28:34.431480Z","shell.execute_reply":"2024-04-25T15:28:34.441315Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**DEBERTA**","metadata":{}},{"cell_type":"code","source":"pl_deberta_model = AccentModel(deberta_model, tokenizer, 1e-5)\ncheckpoint_callback = ModelCheckpoint(monitor='validation_loss', mode='min', save_top_k=3)\nlr_monitor = LearningRateMonitor(logging_interval='step')\nwandb_logger = WandbLogger(project='hw9_nn_kirichenko', name=\"deberta_model_half_data_15epoch\", job_type='train', save_dir='/kaggle/working/lightning_logs/version_1')\ntrainer = pl.Trainer(max_epochs=15, logger=wandb_logger, accelerator=device.type, callbacks=[checkpoint_callback, lr_monitor])","metadata":{"execution":{"iopub.status.busy":"2024-04-24T10:15:47.214816Z","iopub.execute_input":"2024-04-24T10:15:47.215650Z","iopub.status.idle":"2024-04-24T10:15:47.897946Z","shell.execute_reply.started":"2024-04-24T10:15:47.215616Z","shell.execute_reply":"2024-04-24T10:15:47.897118Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.fit(model=pl_deberta_model,\n           train_dataloaders=train_dataloader,\n           val_dataloaders=val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T10:15:50.489945Z","iopub.execute_input":"2024-04-24T10:15:50.490594Z","iopub.status.idle":"2024-04-24T12:51:52.297446Z","shell.execute_reply.started":"2024-04-24T10:15:50.490561Z","shell.execute_reply":"2024-04-24T12:51:52.296353Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtoly-kiri4enko\u001b[0m (\u001b[33mbstu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/working/lightning_logs/version_1/wandb/ wasn't writable, using system temp directory.\nwandb: WARNING Path /kaggle/working/lightning_logs/version_1/wandb/ wasn't writable, using system temp directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/tmp/wandb/run-20240424_101550-adj42qz7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bstu/hw9_nn_kirichenko/runs/adj42qz7/workspace' target=\"_blank\">deberta_model_half_data_15epoch</a></strong> to <a href='https://wandb.ai/bstu/hw9_nn_kirichenko' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bstu/hw9_nn_kirichenko' target=\"_blank\">https://wandb.ai/bstu/hw9_nn_kirichenko</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bstu/hw9_nn_kirichenko/runs/adj42qz7/workspace' target=\"_blank\">https://wandb.ai/bstu/hw9_nn_kirichenko/runs/adj42qz7/workspace</a>"},"metadata":{}},{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type                            | Params\n----------------------------------------------------------\n0 | model | DebertaV2ForTokenClassification | 36.3 M\n----------------------------------------------------------\n36.3 M    Trainable params\n0         Non-trainable params\n36.3 M    Total params\n145.388   Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153719b6e3e9451dbbf012cd87e6dd78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=15` reached.\n","output_type":"stream"}]},{"cell_type":"code","source":"pl_deberta_model.eval()\npl_deberta_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:52:41.915242Z","iopub.execute_input":"2024-04-24T12:52:41.915639Z","iopub.status.idle":"2024-04-24T12:52:41.972903Z","shell.execute_reply.started":"2024-04-24T12:52:41.915605Z","shell.execute_reply":"2024-04-24T12:52:41.971684Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"AccentModel(\n  (model): DebertaV2ForTokenClassification(\n    (deberta): DebertaV2Model(\n      (embeddings): DebertaV2Embeddings(\n        (word_embeddings): Embedding(128100, 256, padding_idx=0)\n        (position_embeddings): Embedding(512, 256)\n        (LayerNorm): LayerNorm((256,), eps=1e-07, elementwise_affine=True)\n        (dropout): StableDropout()\n      )\n      (encoder): DebertaV2Encoder(\n        (layer): ModuleList(\n          (0-3): 4 x DebertaV2Layer(\n            (attention): DebertaV2Attention(\n              (self): DisentangledSelfAttention(\n                (query_proj): Linear(in_features=256, out_features=256, bias=True)\n                (key_proj): Linear(in_features=256, out_features=256, bias=True)\n                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n                (pos_dropout): StableDropout()\n                (dropout): StableDropout()\n              )\n              (output): DebertaV2SelfOutput(\n                (dense): Linear(in_features=256, out_features=256, bias=True)\n                (LayerNorm): LayerNorm((256,), eps=1e-07, elementwise_affine=True)\n                (dropout): StableDropout()\n              )\n            )\n            (intermediate): DebertaV2Intermediate(\n              (dense): Linear(in_features=256, out_features=1024, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): DebertaV2Output(\n              (dense): Linear(in_features=1024, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n        )\n        (rel_embeddings): Embedding(1024, 256)\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=256, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def compute_accuracy(model, test_dataloader):\n    accuracy = 0\n    for i, (batch) in enumerate(test_dataloader):\n        batch = batch.to(device)\n        model_output = pl_model(batch)\n        accuracy += torch.sum(torch.argmax(model_output[:, :, 1], dim=1) == torch.argmax(batch['labels'], dim=1)) / len(torch.argmax(batch['labels'], dim=1))\n\n    return accuracy / i\n\ncompute_accuracy(pl_deberta_model, test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:52:55.052315Z","iopub.execute_input":"2024-04-24T12:52:55.053001Z","iopub.status.idle":"2024-04-24T12:54:18.192679Z","shell.execute_reply.started":"2024-04-24T12:52:55.052969Z","shell.execute_reply":"2024-04-24T12:54:18.191515Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor(0.7772, device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"**ALBERT**","metadata":{}},{"cell_type":"code","source":"from transformers import AlbertConfig, AlbertForTokenClassification\n\nalbert_config = AlbertConfig(hidden_size=256,\n                        intermediate_size=1024,\n                        max_length=40,\n                        num_attention_heads=8,\n                        num_hidden_layers=4,\n                        position_biased_input=True,\n                        relative_attention=True)\n\nalbert_model = AlbertForTokenClassification(albert_config)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:06:27.853537Z","iopub.execute_input":"2024-04-24T13:06:27.854551Z","iopub.status.idle":"2024-04-24T13:06:27.995982Z","shell.execute_reply.started":"2024-04-24T13:06:27.854516Z","shell.execute_reply":"2024-04-24T13:06:27.994843Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pl_albert_model = AccentModel(albert_model, tokenizer, 1e-5)\ncheckpoint_callback = ModelCheckpoint(monitor='validation_loss', mode='min', save_top_k=3)\nlr_monitor = LearningRateMonitor(logging_interval='step')\nwandb_logger = WandbLogger(project='hw9_nn_kirichenko', name=\"albert_model_half_data_15epoch\", job_type='train', save_dir='/kaggle/working/lightning_logs/version_1')\ntrainer = pl.Trainer(max_epochs=15, logger=wandb_logger, accelerator=device.type, callbacks=[checkpoint_callback, lr_monitor])","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:06:47.473283Z","iopub.execute_input":"2024-04-24T13:06:47.473857Z","iopub.status.idle":"2024-04-24T13:06:48.153080Z","shell.execute_reply.started":"2024-04-24T13:06:47.473820Z","shell.execute_reply":"2024-04-24T13:06:48.152176Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.fit(model=pl_albert_model,\n           train_dataloaders=train_dataloader,\n           val_dataloaders=val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:06:51.870970Z","iopub.execute_input":"2024-04-24T13:06:51.871337Z","iopub.status.idle":"2024-04-24T15:37:33.011909Z","shell.execute_reply.started":"2024-04-24T13:06:51.871311Z","shell.execute_reply":"2024-04-24T15:37:33.010713Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtoly-kiri4enko\u001b[0m (\u001b[33mbstu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/working/lightning_logs/version_1/wandb/ wasn't writable, using system temp directory.\nwandb: WARNING Path /kaggle/working/lightning_logs/version_1/wandb/ wasn't writable, using system temp directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/tmp/wandb/run-20240424_130651-d5bgb1tv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bstu/hw9_nn_kirichenko/runs/d5bgb1tv/workspace' target=\"_blank\">albert_model_half_data_15epoch</a></strong> to <a href='https://wandb.ai/bstu/hw9_nn_kirichenko' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bstu/hw9_nn_kirichenko' target=\"_blank\">https://wandb.ai/bstu/hw9_nn_kirichenko</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bstu/hw9_nn_kirichenko/runs/d5bgb1tv/workspace' target=\"_blank\">https://wandb.ai/bstu/hw9_nn_kirichenko/runs/d5bgb1tv/workspace</a>"},"metadata":{}},{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type                         | Params\n-------------------------------------------------------\n0 | model | AlbertForTokenClassification | 4.7 M \n-------------------------------------------------------\n4.7 M     Trainable params\n0         Non-trainable params\n4.7 M     Total params\n18.917    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9eac58f817a4e0b973f445291f8a5c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=15` reached.\n","output_type":"stream"}]},{"cell_type":"code","source":"pl_albert_model.eval()\npl_albert_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T15:37:53.864001Z","iopub.execute_input":"2024-04-24T15:37:53.864924Z","iopub.status.idle":"2024-04-24T15:37:53.882715Z","shell.execute_reply.started":"2024-04-24T15:37:53.864880Z","shell.execute_reply":"2024-04-24T15:37:53.881844Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"AccentModel(\n  (model): AlbertForTokenClassification(\n    (albert): AlbertModel(\n      (embeddings): AlbertEmbeddings(\n        (word_embeddings): Embedding(30000, 128, padding_idx=0)\n        (position_embeddings): Embedding(512, 128)\n        (token_type_embeddings): Embedding(2, 128)\n        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n      (encoder): AlbertTransformer(\n        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=256, bias=True)\n        (albert_layer_groups): ModuleList(\n          (0): AlbertLayerGroup(\n            (albert_layers): ModuleList(\n              (0): AlbertLayer(\n                (full_layer_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n                (attention): AlbertAttention(\n                  (query): Linear(in_features=256, out_features=256, bias=True)\n                  (key): Linear(in_features=256, out_features=256, bias=True)\n                  (value): Linear(in_features=256, out_features=256, bias=True)\n                  (attention_dropout): Dropout(p=0, inplace=False)\n                  (output_dropout): Dropout(p=0, inplace=False)\n                  (dense): Linear(in_features=256, out_features=256, bias=True)\n                  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n                )\n                (ffn): Linear(in_features=256, out_features=1024, bias=True)\n                (ffn_output): Linear(in_features=1024, out_features=256, bias=True)\n                (activation): NewGELUActivation()\n                (dropout): Dropout(p=0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=256, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def compute_accuracy(model, test_dataloader):\n    accuracy = 0\n    for i, (batch) in enumerate(test_dataloader):\n        batch = batch.to(device)\n        model_output = model(batch)\n        accuracy += torch.sum(torch.argmax(model_output[:, :, 1], dim=1) == torch.argmax(batch['labels'], dim=1)) / len(torch.argmax(batch['labels'], dim=1))\n\n    return accuracy / i\n\ncompute_accuracy(pl_albert_model, test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:45:13.542352Z","iopub.execute_input":"2024-04-25T15:45:13.542778Z","iopub.status.idle":"2024-04-25T15:45:13.551837Z","shell.execute_reply.started":"2024-04-25T15:45:13.542734Z","shell.execute_reply":"2024-04-25T15:45:13.550672Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**BERT**","metadata":{}},{"cell_type":"code","source":"from transformers import BertConfig, BertForTokenClassification\n\nbert_config = BertConfig(hidden_size=256,\n                        intermediate_size=1024,\n                        max_length=40,\n                        num_attention_heads=8,\n                        num_hidden_layers=4,\n                        position_biased_input=True,\n                        relative_attention=True)\n\nbert_model = BertForTokenClassification(bert_config)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:53.615254Z","iopub.execute_input":"2024-04-25T15:28:53.616026Z","iopub.status.idle":"2024-04-25T15:28:54.932486Z","shell.execute_reply.started":"2024-04-25T15:28:53.615993Z","shell.execute_reply":"2024-04-25T15:28:54.931561Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"pl_bert_model = AccentModel(bert_model, tokenizer, 1e-5)\ncheckpoint_callback = ModelCheckpoint(monitor='validation_loss', mode='min', save_top_k=3)\nlr_monitor = LearningRateMonitor(logging_interval='step')\nwandb_logger = WandbLogger(project='hw9_nn_kirichenko', name=\"bert_model_half_data_10epoch\", job_type='train', save_dir='/kaggle/working/lightning_logs/version_1')\ntrainer = pl.Trainer(max_epochs=15, logger=wandb_logger, accelerator=device.type, callbacks=[checkpoint_callback, lr_monitor])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:56.915512Z","iopub.execute_input":"2024-04-25T15:28:56.918318Z","iopub.status.idle":"2024-04-25T15:28:57.643153Z","shell.execute_reply.started":"2024-04-25T15:28:56.918284Z","shell.execute_reply":"2024-04-25T15:28:57.642144Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.fit(model=pl_bert_model,\n           train_dataloaders=train_dataloader,\n           val_dataloaders=val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:28:59.361198Z","iopub.execute_input":"2024-04-25T15:28:59.361585Z","iopub.status.idle":"2024-04-25T15:45:13.539650Z","shell.execute_reply.started":"2024-04-25T15:28:59.361554Z","shell.execute_reply":"2024-04-25T15:45:13.538346Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtoly-kiri4enko\u001b[0m (\u001b[33mbstu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/working/lightning_logs/version_1/wandb/ wasn't writable, using system temp directory.\nwandb: WARNING Path /kaggle/working/lightning_logs/version_1/wandb/ wasn't writable, using system temp directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/tmp/wandb/run-20240425_152859-pfqetnct</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bstu/hw9_nn_kirichenko/runs/pfqetnct/workspace' target=\"_blank\">bert_model_half_data_10epoch</a></strong> to <a href='https://wandb.ai/bstu/hw9_nn_kirichenko' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bstu/hw9_nn_kirichenko' target=\"_blank\">https://wandb.ai/bstu/hw9_nn_kirichenko</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bstu/hw9_nn_kirichenko/runs/pfqetnct/workspace' target=\"_blank\">https://wandb.ai/bstu/hw9_nn_kirichenko/runs/pfqetnct/workspace</a>"},"metadata":{}},{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type                       | Params\n-----------------------------------------------------\n0 | model | BertForTokenClassification | 11.1 M\n-----------------------------------------------------\n11.1 M    Trainable params\n0         Non-trainable params\n11.1 M    Total params\n44.421    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6807d89a3da42a1be6c723e99fc8664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=15` reached.\n","output_type":"stream"}]},{"cell_type":"code","source":"pl_bert_model.eval()\npl_bert_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:45:42.476783Z","iopub.execute_input":"2024-04-25T15:45:42.477162Z","iopub.status.idle":"2024-04-25T15:45:42.509822Z","shell.execute_reply.started":"2024-04-25T15:45:42.477131Z","shell.execute_reply":"2024-04-25T15:45:42.508938Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"AccentModel(\n  (model): BertForTokenClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 256, padding_idx=0)\n        (position_embeddings): Embedding(512, 256)\n        (token_type_embeddings): Embedding(2, 256)\n        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-3): 4 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=256, out_features=256, bias=True)\n                (key): Linear(in_features=256, out_features=256, bias=True)\n                (value): Linear(in_features=256, out_features=256, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=256, out_features=256, bias=True)\n                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=256, out_features=1024, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=1024, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=256, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"compute_accuracy(pl_bert_model, test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:45:44.458909Z","iopub.execute_input":"2024-04-25T15:45:44.459859Z","iopub.status.idle":"2024-04-25T15:45:53.768981Z","shell.execute_reply.started":"2024-04-25T15:45:44.459820Z","shell.execute_reply":"2024-04-25T15:45:53.767860Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor(0.4521, device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"Deberta, Albert обучались с одинаковыми параметрами и одинаковое количество эпох на одинаковых данных. Bert пришлось обучить на урезанном количестве данных, так как google colab дропает обучение спустя полтора часа, а на kaggle закончились вычислительные единицы.\n\nТочность моделей полученную при обучении моделей можно улучшить, если подобрать получше параметры для конфигов (например увеличить размер скрытых слоев или их количество).","metadata":{}}]}